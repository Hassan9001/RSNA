# @package _global_
data:
  module:
    _target_: datasets.AVM_T1+C_Data.AVM_T1C_DataModule
    name: AVM_T1+C
    data_root_dir: /hpf/projects/jquon/sumin/nnssl_dataset/nnssl_preprocessed/Dataset003_AVM_T1+C
    batch_size: 8
    train_transforms:
      _target_: augmentation.policies.batchgenerators.get_training_transforms
      patch_size: ${data.patch_size}
      rotation_for_DA: 0.523599
      mirror_axes: [0,1,2]
      do_dummy_2d_data_aug: False
    test_transforms: 
      _target_: augmentation.policies.batchgenerators.get_test_transforms
      patch_size: ${data.patch_size}
      do_dummy_2d_data_aug: False
  cv:
    k: 5

  num_classes: 2
  patch_size: [160, 160, 160]

model:
  task: 'Classification_Focal_Loss' #'Classification'
  subtask: 'multiclass' 
  cifar_size: False
  input_channels: 1
  input_dim: 3
  input_shape: ${data.patch_size}
  optimizer: AdamW
  lr: 0.001
  warmstart: 20
  warmstart2: 20
  weight_decay: 1e-2
  finetune_method: full_sawtooth


trainer:
  logger:
    project: nnSSL AVM_T1+C Classification #project name for wandb
    name: scratch_focalloss_lr_1e-3_5folds_bsize8 #experiment name for wandb (but its overwritten by slurm script)
  accumulate_grad_batches: 48
  max_epochs: 300
  sync_batchnorm: True
  callbacks: 
    checkpoint:
      filename: 'epoch{epoch}-Val_loss={Val/loss:.2f}-Val_bal_acc={Val/Balanced_Accuracy:.2f}'
      mode: 'max'
      monitor: 'Val/Balanced_Accuracy'

metrics:
  # - 'mae' 
  # - 'mse'
  - 'f1'
  - 'balanced_acc'
  - 'ap'
  - 'auroc'