[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
output_subdir: ${exp_dir}/${data.module.name}/checkpoints/${make_group_name:}
override hydra/hydra_logging: disable
override hydra/job_logging: disable
seed: false
model:
  optimizer: AdamW
  sam: false
  adaptive_sam: false
  lr: 0.001
  nesterov: false
  scheduler: CosineAnneal
  T_max: ${trainer.max_epochs}
  warmstart: 20
  warmstart2: 20
  mixup: false
  mixup_alpha: 0.2
  weight_decay: 0.01
  undecay_norm: false
  label_smoothing: 0.0
  stochastic_depth: false
  resnet_dropout: 0.0
  squeeze_excitation: false
  apply_shakedrop: false
  zero_init_residual: false
  input_dim: 3
  input_channels: 1
  task: Classification
  subtask: multiclass
  metric_computation_mode: epochwise
  result_plot: val
  num_classes: ${data.num_classes}
  metrics: ${metrics}
  epochs: ${trainer.max_epochs}
  name: ${model_name_extractor:${model._target_}}
  compile: false
  classification_head_dropout: 0.2
  token_aggregation_method: null
  pretrained: false
  finetune_method: full_sawtooth
  save_preds: true
  _target_: models.resenc.ResEncoder_Classifier
  chpt_path: null
  cifar_size: false
  input_shape: ${data.patch_size}
data:
  module:
    random_batches: false
    use_weighted_sampling: true
    num_workers: 12
    prepare_data_per_node: false
    fold: 4
    _target_: datasets.AVM_T1+C_Data.AVM_T1C_DataModule
    name: AVM_T1+C
    data_root_dir: /hpf/projects/jquon/sumin/nnssl_dataset/nnssl_preprocessed/Dataset003_AVM_T1+C
    batch_size: 8
    train_transforms:
      _target_: augmentation.policies.batchgenerators.get_training_transforms
      patch_size: ${data.patch_size}
      rotation_for_DA: 0.523599
      mirror_axes:
      - 0
      - 1
      - 2
      do_dummy_2d_data_aug: false
    test_transforms:
      _target_: augmentation.policies.batchgenerators.get_test_transforms
      patch_size: ${data.patch_size}
      do_dummy_2d_data_aug: false
  num_classes: 2
  cv:
    k: 5
  patch_size:
  - 160
  - 160
  - 160
metrics:
- f1
- balanced_acc
- ap
- auroc
val_only: false
trainer:
  _target_: lightning.pytorch.Trainer
  callbacks:
  - _target_: lightning.pytorch.callbacks.TQDMProgressBar
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: epoch
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ???
    filename: epoch{epoch}-Val_loss={Val/loss:.2f}-Val_bal_acc={Val/Balanced_Accuracy:.2f}
    auto_insert_metric_name: false
    monitor: Val/Balanced_Accuracy
    mode: max
    save_top_k: 5
    save_last: true
    save_weights_only: false
  devices: 1
  accelerator: gpu
  sync_batchnorm: true
  enable_checkpointing: true
  max_epochs: 300
  benchmark: true
  deterministic: false
  precision: 16-mixed
  enable_progress_bar: true
  strategy: ddp_find_unused_parameters_true
  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
    save_dir: ${exp_dir}/${data.module.name}
    offline: false
    project: nnSSL AVM_T1+C Classification
    group: 150dddec-6656-4222-9943-b5c275657671
    name: scratch_balanced_Fold4_lr_1e-3_5folds_bsize8
  num_sanity_val_steps: 0
  accumulate_grad_batches: 48
  log_every_n_steps: 50
exp_dir: ${oc.env:EXPERIMENT_LOCATION}

training fold: 4
[SpatialTransform( patch_size = [160, 160, 160], patch_center_dist_from_border = [0, 0, 0], random_crop = False, p_elastic_deform = 0, elastic_deform_scale = (0, 0.2), elastic_deform_magnitude = (0, 0.2), p_rotation = 0.2, rotation = 0.523599, p_scaling = 0.2, scaling = (0.7, 1.4), p_synchronize_scaling_across_axes = 1, p_synchronize_def_scale_across_axes = 0, bg_style_seg_sampling = False, mode_seg = 'bilinear', border_mode_seg = 'zeros', center_deformation = True, padding_mode_image = 'zeros' ), RandomTransform(p=0.1, transform=GaussianNoiseTransform( noise_variance = (0, 0.1), p_per_channel = 1, synchronize_channels = True )), RandomTransform(p=0.2, transform=GaussianBlurTransform( blur_sigma = (0.5, 1.0), benchmark = True, synchronize_channels = False, synchronize_axes = False, p_per_channel = 0.5, benchmark_use_fft = {}, benchmark_num_runs = 9 )), RandomTransform(p=0.15, transform=MultiplicativeBrightnessTransform( multiplier_range = BGContrast(contrast_range=(0.75, 1.25)), synchronize_channels = False, p_per_channel = 1 )), RandomTransform(p=0.15, transform=ContrastTransform( contrast_range = BGContrast(contrast_range=(0.75, 1.25)), preserve_range = True, synchronize_channels = False, p_per_channel = 1 )), RandomTransform(p=0.25, transform=SimulateLowResolutionTransform( scale = (0.5, 1), synchronize_channels = False, synchronize_axes = True, ignore_axes = None, allowed_channels = None, p_per_channel = 0.5, upmodes = {1: 'linear', 2: 'bilinear', 3: 'trilinear'} )), RandomTransform(p=0.1, transform=GammaTransform( gamma = BGContrast(contrast_range=(0.7, 1.5)), p_invert_image = 1, synchronize_channels = False, p_per_channel = 1, p_retain_stats = 1 )), RandomTransform(p=0.3, transform=GammaTransform( gamma = BGContrast(contrast_range=(0.7, 1.5)), p_invert_image = 0, synchronize_channels = False, p_per_channel = 1, p_retain_stats = 1 )), MirrorTransform( allowed_axes = [0, 1, 2] )]
[INFO] Using CosineAnnealingLR_DoubleWarmstart: warmstart1=20, warmstart2=20, T_max=300
Initializing weighted random sampler...
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mscratch_balanced_Fold4_lr_1e-3_5folds_bsize8[0m at: [34mhttps://wandb.ai/hassan9000-m31-biomedical-ai/nnSSL%20AVM_T1%2BC%20Classification/runs/2qje2p24[0m
[1;34mwandb[0m: Find logs at: [1;35m../../sumin/nnssl_dataset/nnssl_results/Dataset003_AVM_T1+C/scratch_balanced_lr_1e-3_5folds_bsize8/AVM_T1+C/wandb/run-20250915_141230-2qje2p24/logs[0m
